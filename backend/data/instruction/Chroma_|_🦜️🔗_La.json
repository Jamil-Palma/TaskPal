{
  "task": "Chroma: Embeddings & Retrieval \n",
  "steps": [
    "Step 1: Install Chroma using the command: 'pip install chroma-client'",
    "Step 2: Create a Chroma client using the following code:\n```python\nfrom chromadb.client import Client\nclient = Client(path='./chroma_db')\n```\nThis will create a Chroma client that saves data to the specified directory.",
    "Step 3: Load data into Chroma. In this example, we'll load text chunks from a State of the Union address and embed them using an Open-source embedding model:\n```python\nfrom langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\n\n# Load the text\nwith open('state_of_the_union.txt', 'r') as f:\n    text = f.read()\n\n# Split the text into chunks\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\nchunks = text_splitter.split_text(text)\n\n# Embed the chunks\nembeddings = OpenAIEmbeddings()\nembeddings = embeddings.embed_documents(chunks)\n\n# Load the chunks into Chroma\nvectordb = Chroma.from_embeddings(embeddings, client=client)\n```",
    "Step 4: Query the data in Chroma:\n```python\nquery = 'What did the president say about the economy?'\nresults = vectordb.similarity_search(query, k=3)\nprint(results)\n```\nThis will return the top 3 most similar chunks of text to the query.",
    "Step 5: Access Chroma methods directly using the `_collection` attribute. For example, to get the collection name:\n```python\ncollection_name = vectordb._collection.name\n```",
    "Step 6:  Save Chroma data to disk by initializing the client with a directory path:\n```python\nfrom chromadb.client import Client\nclient = Client(path='./chroma_db')\n```\nChroma will automatically save the data to the specified directory.",
    "Step 7: Create a Chroma client and pass it to LangChain for easier access:\n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms import OpenAI\n\nclient = Client(path='./chroma_db')\nvectordb = Chroma(client=client, embedding_function=OpenAIEmbeddings())\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type='stuff', retriever=vectordb.as_retriever())\nqa.run('What did the president say about the economy?')\n```",
    "Step 8: Specify the collection name for LangChain to use:\n```python\nclient = Client(path='./chroma_db')\nvectordb = Chroma(client=client, embedding_function=OpenAIEmbeddings(), collection_name='my_collection')\n```",
    "Step 9: Run the Chroma Server in a Docker container and connect to it with a client:\n```bash\n# Clone the Chroma repository\ngit clone https://github.com/chroma-core/chroma.git\n\n# Build the Docker image\ncd chroma\ndocker build -t chroma . \n\n# Run the Docker container\ndocker run -p 8000:8000 chroma\n\n# Connect to the Chroma Server\nfrom chromadb.client import Client\nclient = Client(host='localhost', port=8000)\n```",
    "Step 10: Use multiple collections in Chroma:\n```python\nclient = Client(path='./chroma_db')\n\n# Create two collections\ncollection1 = client.get_or_create_collection(name='collection1')\ncollection2 = client.get_or_create_collection(name='collection2')\n```",
    "Step 11: Use LangChain with multiple Chroma collections by specifying the collection name:\n```python\nclient = Client(path='./chroma_db')\nvectordb = Chroma(client=client, embedding_function=OpenAIEmbeddings(), collection_name='my_collection')\n```",
    "Step 12: Update and delete data in Chroma using IDs:\n```python\n# Update data\nclient.update(collection_name='my_collection', ids=['doc1'], documents=['Updated document text'], metadata={'source': 'updated'}) \n\n# Delete data\nclient.delete(collection_name='my_collection', ids=['doc1'])\n```",
    "Step 13: Use OpenAIEmbeddings for embedding text:\n```python\nfrom langchain.embeddings.openai import OpenAIEmbeddings\n\nembeddings = OpenAIEmbeddings()\n```",
    "Step 14: Use Chroma as a retriever with similarity search:\n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\n\nvectordb = Chroma.from_documents(documents, embedding_function=OpenAIEmbeddings())\nretriever = vectordb.as_retriever()\n```",
    "Step 15: Use Maximum Marginal Relevance (MMR) for retrieving documents:\n```python\nfrom langchain.chains import RetrievalQA\nfrom langchain.vectorstores import Chroma\nfrom langchain.retrievers.mmr import MMREngine\n\nvectordb = Chroma.from_documents(documents, embedding_function=OpenAIEmbeddings())\nmmr = MMREngine(retriever=vectordb.as_retriever(), k=3)\nqa = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type='stuff', retriever=mmr)\n```",
    "Step 16: Filter collections based on metadata:\n```python\nclient = Client(path='./chroma_db')\ncollection = client.get_or_create_collection(name='my_collection')\n\n# Filter collection based on metadata\nfiltered_collection = collection.get(where={'source': 'article'}) \n```"
  ],
  "summary": "Chroma is an open-source vector database designed for developer productivity, focused on ease of use and integration with tools like LangChain. It allows for storing and querying embedded data, enabling applications like semantic search and retrieval. This article demonstrates various ways to use Chroma, including loading data, querying, saving to disk, and interacting with LangChain. It also covers advanced features like updating and deleting data, using OpenAIEmbeddings, and employing techniques like MMR for retrieval. The article emphasizes Chroma's flexibility and potential for building powerful applications with its integration capabilities. \n"
}